[TOC]



# 一、ATTENTION IS ALL U NEED

## 1、ML和DL的区别

> - 神经网络是一个算法，也是一个`提取特征的手段`
>
> - 机器学习和深度学习的区别
>
>   - **特征工程**：
>
>     - 机器学习：需要`人工`地选择数据和提取特征、`人工`地选择一个算法、`人工`地得出结果（歪歪斜斜的每页上都写着“智能”几个字，我横竖睡不着，仔细看了半夜，才从字缝里看出来，满本上都写着两个字“人工"！）
>
>       有种感觉——整个学习过程 = 实现了一个数学公式
>
>     - 深度学习：去除ML的人工步骤，让机器去提取和选择特征，真正实现智能
>
>       

<img src="深度学习.assets/image-20230516180343682.png" alt="image-20230516180343682" style="zoom:67%;" /> 



## 2、特征工程的作用

> - 特征工程
>
>   - `数据的特征决定了模型的上限`，因此对吧，`预处理和特征的提取`是最core的
>
>     我们的`算法和参数选择`就是为了*逼近这个上限*
>
>   - 所以这玩意挺重要的，上限很重要嘛

<img src="深度学习.assets/image-20230516182533461.png" alt="image-20230516182533461" style="zoom:67%;" /> 







## 3、应用

> - CV、自动驾驶、人脸识别、NLP、换脸

<img src="深度学习.assets/image-20230516184254706.png" alt="image-20230516184254706" style="zoom: 67%;" /> 



> - 带你走一次图像分类

<img src="深度学习.assets/image-20230516185539385.png" alt="image-20230516185539385" style="zoom:67%;" /> 

<img src="深度学习.assets/image-20230516185708320.png" alt="image-20230516185708320" style="zoom:67%;" /> 



CV中的挑战

<img src="深度学习.assets/image-20230516185810086.png" alt="image-20230516185810086" style="zoom:67%;" /> 

<img src="深度学习.assets/image-20230516185828153.png" alt="image-20230516185828153" style="zoom:67%;" /> 





## 4、K近邻算法

> - `K近邻算法`
>   - 什么玩意？
>     - 这个算法的说法就是：判断一个未知的事物的时候，通过`“离它近”`的已知事物推算它的性质

> - 下面的K=3和K=5什么意思？
>   - K=3就是那个实线圈，里面有两个triangle和一个square，所以结果为绿色的圈圈为triangle
>   - 时K=5就是那个虚线圈，里面有两个triangle和三个square，所以结果为绿色的圈圈为square

<img src="深度学习.assets/image-20230516212643311.png" alt="image-20230516212643311" style="zoom:67%;" /> 



**4.1、算法流程**

> - 就是把离当前点近的k个点选出来
>
>   找出这k个点中出现频率最高的类别(如triangle出现最高)
>
>   那么这个点就是此类别(triangle)

<img src="深度学习.assets/image-20230516214903183.png" alt="image-20230516214903183" style="zoom:67%;" /> 



**4.2、性能分析**

<img src="深度学习.assets/image-20230516215206074.png" alt="image-20230516215206074" style="zoom:67%;" /> 



**4.3、实例**

> - 计算距离的差值，得到矩阵

<img src="深度学习.assets/image-20230516215514423.png" alt="image-20230516215514423" style="zoom:67%;" /> 

<img src="深度学习.assets/image-20230516220406022.png" alt="image-20230516220406022" style="zoom:67%;" /> 





**4.4、缺点**

> - 在picture classification时，K近邻算法`无法做到关注主体部分`，洒洒水啦

<img src="深度学习.assets/image-20230516220459849.png" alt="image-20230516220459849" style="zoom:67%;" /> 





# 二、前向-后向传播神经网络

## 1、得分函数

> - 解释一下得分函数?
>   $$
>   f(x,W,b) = Wx+b\\
>   x:image，即原图片\\
>   W:parameter，权重\\
>   b:threshold，偏置\\
>   (这玩意就是判别x在不同类别的得分，以求得x所属的类别)\\
>   $$
>
> - 亮个相吧，x、W、b
>
>   - `x`用矩阵表示，32×32×3的图片，可以`用3072×1的矩阵表示`，`每个元素即为一个像素点`
>
>   - `W`用矩阵表示，32×32×3的图片，`每个像素点都有不同的权重，可以用1×3072的矩阵表示`
>
>   - `b`用矩阵表示，图片要判别在十个类别的得分，`每个类别有一个阈值，所以b用10×1的矩阵表示`
>
>     (上面的可以结合下面的图片看得清楚一点)
>
> - 为什么要有权重？
>
>   - 因为如何你想测试这个图片是不是猫猫，那么就`需要特点关注猫的特征(如眼睛、耳朵等)`
>
>     所以这张图片*有些像素是没有意义的，有一些像素是很重要的*。因此需要权重来区分
>
> - 为什么要有偏置？
>
>   - 希腊奶哦，后面来补

<img src="深度学习.assets/image-20230516221559530.png" alt="image-20230516221559530" style="zoom:67%;" /> 



> - 实例

<img src="深度学习.assets/image-20230516223916358.png" alt="image-20230516223916358" style="zoom:67%;" /> 



## 2、激活函数

> - Sigmoid求梯度不行啊
>   $$
>   \frac{\partial L}{\partial \sigma}\\
>   \sigma太大了，最后式子结果就为0了，就无限实现BP梯度了
>   $$
>
> - Relu挺好的，梯度为1，不会出现这种情况

<img src="深度学习.assets/image-20230520201701283.png" alt="image-20230520201701283" style="zoom:67%;" /> 





## 3、损失函数

> - 啥啥啥啊这儿是？
>   - 衡量`得到的结果`与`正确分类`的`差值`

<img src="深度学习.assets/image-20230516224219024.png" alt="image-20230516224219024" style="zoom:67%;" /> 





### 3.1、实例

> - 损失函数
>   $$
>   L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+1\right)\\
>   $$
>
> - 为什么要用<img src="深度学习.assets/image-20230516232009486.png" alt="image-20230516232009486" style="zoom:67%;" /> ？什么max？什么减减加加？
>
>   - <img src="深度学习.assets/image-20230516232011287.png" alt="image-20230516232011287" style="zoom:67%;" />  ：除了目的类别外，对于每一个类别的得分与目的类别得分之差(为正数即有问题)求和，把全部的情况求和
>
>     ​	    的含义`判断当前是否存在有没分类好的情况，即出现正数`；*如果求和结果为0，那么最高分为目的类别，分类成功*
>
>   - <img src="深度学习.assets/image-20230516231406691.png" alt="image-20230516231406691" style="zoom:80%;" /> ：`离达到目标类别的差值`
>
>     <img src="深度学习.assets/image-20230516231420522.png" alt="image-20230516231420522" style="zoom:80%;" /> ：分类中的最高分
>
>     <img src="深度学习.assets/image-20230516231437947.png" alt="image-20230516231437947" style="zoom:80%;" /> ：目标分类的得分
>
>     <img src="深度学习.assets/image-20230516231349225.png" alt="image-20230516231349225" style="zoom:80%;" /> 即为离目标的差值
>
>   - `max(0,difference)：`
>
>     如果目标分类不是最高分，那么difference就是正数，
>
>     与0求max后，最终的结果为difference，表明仍有差值；
>
>     只有`当目标类别得分为最高时，difference为0，最终结果为0`，表明没有差值
>
> - **为什么要+1？**
>
>   - 因为当<img src="深度学习.assets/image-20230516231420522.png" alt="image-20230516231420522" style="zoom:80%;" />略小于<img src="深度学习.assets/image-20230516231437947.png" alt="image-20230516231437947" style="zoom:80%;" /> ：目标分类的得分时，difference也为负(如3.15-3.20)，但是`两者差别不大，说明这次分类其实没那么灵，还是没能完全区分开其他类别`
>
>     加上1以后，表明只有当<img src="深度学习.assets/image-20230516231027828.png" alt="image-20230516231027828" style="zoom:80%;" /> < 0时才分类成功
>
>     此时<img src="深度学习.assets/image-20230516231155022.png" alt="image-20230516231155022" style="zoom:80%;" /> ，即目的类别和其他类别`区分度足够大`才算没有损失

<img src="深度学习.assets/image-20230516224915994.png" alt="image-20230516224915994" style="zoom: 67%;" /> 





### 3.2、正则化惩罚项

> - 权值矩阵也是影响模型的关键

<img src="深度学习.assets/image-20230519105838301.png" alt="image-20230519105838301" style="zoom:67%;" /> 



> - 正则化惩罚项干嘛的？
>
>   - 正则化通过在模型的损失函数中`引入一个额外的针对参数的惩罚项`，来`防止模型过拟合训练数据`。
>   - 前者为数据所产生的损失
>   - 后者为参数所带来的损失
>     - 与数据没有关系，只考虑到权重参数
>
> - 解释一下该公式
>
>   - <img src="深度学习.assets/image-20230519124120198.png" alt="image-20230519124120198" style="zoom:67%;" /> 
>
>     就是把全部参数矩阵的元素平方求和
>
> - 解释一下这个λ
>
>   - <img src="深度学习.assets/image-20230519124236135.png" alt="image-20230519124236135" style="zoom:67%;" /> 
>
>     **惩罚系数**：控制正则化的强度的超参数。它`决定了正则化项在整个损失函数中的权重`。
>
>     `较大的 λ 值`会增加正则化项的影响，`使模型更加趋向于简单的解决方案`，也就是一些`大一点的权重或者参数会被惩罚为0(记得训练猫猫图片的时候，出现的某个分布十分不均匀的权重矩阵嘛(就在上图)，这就是过拟合，只是在当前data set某个权重突出好一点，换一个data set这个权重突出就起反作用了说不定)`，小一点的就会留下，对于整个模型来说那些突出的尖尖被抹去，更加往下压成一团，更加平滑了。
>
>     `较小的 λ 值`则允许模型`更自由地拟合训练数据，但可能会导致过拟合`。
>
>     
>
> - 通常情况下，`模型不要太复杂，不然容易出现过拟合`
>
>   - 学的太强也不是一件好事

<img src="深度学习.assets/image-20230519113555944.png" alt="image-20230519113555944" style="zoom:67%;" /> 





## 4、迭代法

> - 如图咯，`往y最小`来`从候选值中`选择x的值

 <img src="深度学习.assets/image-20230520001434618.png" alt="image-20230520001434618" style="zoom:67%;" />  



> - 步长变为0.5
> - 步长为0.7的时候，会出现震荡现象

<img src="深度学习.assets/image-20230520001606947.png" alt="image-20230520001606947" style="zoom:67%;" /> 

![image-20230520001713405](深度学习.assets/image-20230520001713405.png) 



> - 为解决震荡现象
>   - 我们让斜率越大时步长越大，斜率越小时步长越小
> - Loss为什么等于那个呢？
>   - 晓不得

<img src="深度学习.assets/image-20230520002122764.png" alt="image-20230520002122764" style="zoom: 80%;" />  





## 5、前向传播

**Forward Propagation Neural Network**

> - 介里我粗略说一下前向传播的流程~
>
>   - f = wx + b，求出得分函数
>
>   - 根据得分函数，求出hinge loss的值
>
>     - 这个hinge loss是指，
>       $$
>       \frac{1}{N}\sum_{i=1}^N 第i个测试用例的损失函数值
>       $$
>       
>
>   - 还不够，损失函数L = hinge loss + 正则化惩罚项 

<img src="深度学习.assets/image-20230519110346298.png" alt="image-20230519110346298" style="zoom:67%;" /> 





### 5.1、Softmax分类器

> - 这玩意什么时候用？
>
>   - 在`得到最后的不同分类的得分值的时候用` 啊西八brother
>
> - 这个SIGMOID函数干嘛的？
>
>   - 因为我们刚才干的是`图片分类`的问题
>
>     所以我们最需要的是`当前图片属于某一类的概率大小`
>
>   - sigmod函数就是把(-∞，+∞)的数，转换为0~1的数，即概率嘛

<img src="深度学习.assets/image-20230519110626766.png" alt="image-20230519110626766" style="zoom:67%;" /> 



> - 讲一下Softmax分类器的流程
>
>   主要是求损失函数不一样
>
>   - (1)已知各得分函数
>
>   - (2)利用指数函数放大
>
>   - (3)求出`不同得分函数的占比`
>
>     <img src="深度学习.assets/image-20230519131557540.png" alt="image-20230519131557540" style="zoom:67%;" /> 
>
>   - (4)利用**log函数**`计算目的类别的损失值`
>
>        为什么要用log函数呢，因为求出目的类别的占比都是0~1的，而占比越小说明误差越大，
>
>        刚好可以用对数函数位于0~1的区间来表示
>
>     - <img src="深度学习.assets/image-20230519131522255.png" alt="image-20230519131522255" style="zoom:67%;" /> 

<img src="深度学习.assets/image-20230519113328200.png" alt="image-20230519113328200" style="zoom:67%;" /> 





### 5.2、细节

> - 理解了前面的process，let's think about the details
>
> - x、w怎么表示？
>
>   - 矩阵，以下图为例
>
>     `x_{1,3}`: `1个样本，三个特征`
>
>     `w_{3,4}`: 权重矩阵，第一行就是x[0\][0]对于hidden layer的四个权重
>
>     `wx_{1,4}`: 1个样本，四个特征。经过求权后，特征变成了四个
>
> - 什么是非线性？
>
>   - 就是每次`求完wx后`，要把结果`经过激活函数变为一个0~1的值(等同于概率)`
>
>     <u>这个概率值再用作下一次的输入</u>

<img src="深度学习.assets/image-20230520184151421.png" alt="image-20230520184151421" style="zoom:67%;" /> 



> - 其实训练过程可以等同于下图公式

<img src="深度学习.assets/image-20230520184436866.png" alt="image-20230520184436866" style="zoom: 50%;" />  





### 5.3、神经元个数的影响

> - 多一点分类效果好一点，但是太多也可能过拟合
> - 那么神经元个数怎么对输出产生影响呢？
>   - 











## 6、反向传播方法

**Back-Propagation Neural Network**

<img src="深度学习.assets/image-20230520001022554.png" alt="image-20230520001022554" style="zoom: 67%;" /> 

> - 这玩意干嘛的？
>   - 更新模型，更新权重矩阵嘛，即Back Propagation

### 6.1、梯度下降/局部梯度

> - cy

<img src="深度学习.assets/image-20230519150425589.png" alt="image-20230519150425589" style="zoom:67%;" /> 

<img src="深度学习.assets/image-20230519150502953.png" alt="image-20230519150502953" style="zoom: 60%;" />  





**局部梯度**

> - ∂z/∂x就是局部梯度

<img src="深度学习.assets/image-20230520003436576.png" alt="image-20230520003436576" style="zoom:67%;" /> 





### 6.2、简单实例

> - 此处求偏导是为什么？
>   - 分别求出x、y、z对输出的影响

<img src="深度学习.assets/image-20230519154907462.png" alt="image-20230519154907462" style="zoom:67%;" /> 



> - 什么叫一步一步传?
>
>   - $$
>     [(x*w_1)*w_2]*w_3=f\\
>     需要先求出w_3，再求出w_2，再求出w_1\\
>     不能跳过w_3直接求出w_1\\
>     其实就是求偏导的链式法则
>     $$

<img src="深度学习.assets/image-20230519155148962.png" alt="image-20230519155148962" style="zoom:67%;" /> 



### 6.3、复杂实例

> - 我现在已经理解了，但我怕你忘了，所以这里补充几个理解的要点
>
>   - <img src="深度学习.assets/image-20230520134427140.png" alt="image-20230520134427140" style="zoom:67%;" /> ——`前缀表达式(1/x即取导数)`——<img src="深度学习.assets/image-20230520134454493.png" alt="image-20230520134454493" style="zoom:67%;" />  
>
>   - `梯度 = 上游梯度 * 局部梯度`
>
>     - `-0.2其实是∂f/∂x`，即∂f/∂z * ∂z/∂x
>
>       ∂f/∂z = -0.53 ——> 用z做输入时求得
>
>       ∂z/∂x = e^(x)，x=-1  ——> 把x当做输入，f=exp
>
>       最终∂f/∂x = -0.53 * e^(-1)
>
>       ![image-20230520134759062](深度学习.assets/image-20230520134759062.png) *举个例子*
>
> - 好的，你目前已经理解了具体过程
>
>   此处我再补充两个地方
>
>   - **Sigmoid函数：**
>
>     - 这个函数有个优点：它的导数为：`(1-原函数)*原函数`
>
>     - 下图的0.2可以直接由0.73求出
>
>       <img src="深度学习.assets/image-20230520135623173.png" alt="image-20230520135623173" style="zoom:67%;" /> 
>
>   - **关于求出梯度干嘛，我目前也不理解**
>
>     - 我逐渐理解一切
>
>       `参数和偏执收到梯度后就可以更新了`
>
>       举例：w_new = w_old - 学习率 * 梯度
>
>     - `为什么要减去梯度呢？`
>
>       我认为有两点，(1)梯度的方向是目标函数增加最快的方向，`注意这是一个矢量的空间`，减去梯度(向量)，即实现了在下降最快的方向下降
>
>       (2)为了解决震荡现象，效率越小，步长越小
>
>       ↓↓↓
>
>       ![image-20230520133422809](深度学习.assets/image-20230520133422809.png)

<img src="深度学习.assets/image-20230520125848449.png" alt="image-20230520125848449" style="zoom:67%;" /> 







## 7、前向和反向传播的特殊运算结构

> 因为X<Y，所以max=Y，所以max对X的偏导为0，对Y的偏导为1，分别乘上上游梯度0.2，可以得到0和0.2-

<img src="深度学习.assets/image-20230520174107007.png" alt="image-20230520174107007" style="zoom:67%;" /> 



> - 如果输入为矩阵呢，你将如何应对呢？
>
> - f是矩阵的`每个元素的平方和`
>
> - 讲一下输出层∂f(q_i)/∂q_i，其中q_i=(wx)_i
>
>   - ∂f(q_i)/∂q_i = 1 * 2q_i
>
>     <img src="深度学习.assets/image-20230520180624263.png" alt="image-20230520180624263" style="zoom: 80%;" /> 
>
> - 如何看出梯度是否有错呢？
>
>   - 我没有说正确，是因为此处我做的是验假
>
>     那就是：`梯度矩阵的行列长度` = `输入矩阵的行列长度`

<img src="深度学习.assets/image-20230520180507576.png" alt="image-20230520180507576" style="zoom:67%;" /> 





## 8、权重的更新迭代

> - 权重更新公式
>   $$
>   w_{(t+1)}=w_{(t)}-\eta\frac{\partial Los}{\partial w}+ \alpha \left[ w_{(t)}-w_{(t-1)}\right]\\
>   \alpha \left[ w_{(t)}-w_{(t-1)}\right]:平滑项，具体不知道什么意思
>   $$
>   

<img src="深度学习.assets/image-20230520182518643.png" alt="image-20230520182518643" style="zoom:67%;" /> 





## 9、预防过拟合

### 9.1、数据预处理

> - 需要两步
>   - 中心化
>   - 对各个维度进行放缩或者扩充

<img src="深度学习.assets/image-20230521011510579.png" alt="image-20230521011510579" style="zoom:67%;" /> 



### 9.2、参数初始化

<img src="深度学习.assets/image-20230521011841500.png" alt="image-20230521011841500" style="zoom:67%;" /> 





### 9.3、DROP-OUT

> - 有这玩意干嘛？
>
>   - 神经元太多，任意过拟合
>
> - 啥啥啥啊这是？
>
>   - 就是每一次训练时，都会把某些隐含层的神经元屏蔽不用
>
>     注意，*只是屏蔽，下次训练可能就会用到了*
>
>     `这样也能训练到全部特征`

<img src="深度学习.assets/image-20230521012514868.png" alt="image-20230521012514868" style="zoom:67%;" /> 







# 三、卷积神经网络

## 1、OUTLINE

**Convolutional Neural Network**

> - 貌似就是把conventional NN的单个神经元换成了多个神经元组成的矩阵

![image-20230918224413014](深度学习.assets/image-20230918224413014.png)



## 2、PROCESS

> - 分别计算出R、G、B Channel的值，将他们累加求和即可得到最终的值

<img src="深度学习.assets/image-20230919105225557.png" alt="image-20230919105225557" style="zoom:67%;" />



## 3、特征图

> - 计算过程是什么？
>
>   - Filter1、Filter2......是多个卷积核
>
>   - 举例：
>
>     三个R、G、B窗口，R .* F1核1 + G .* F1核2 + B .* F1核3 + 偏置 = Output volumes[1, 1, 1]
>
>     其他类推
>
> - 步长是什么？
>
>   - 滑动窗口移动的距离，下图就表明步长为2
>
>     <img src="深度学习.assets/image-20230919113625121.png" alt="image-20230919113625121" style="zoom:50%;" />

<img src="深度学习.assets/image-20230919113327294.png" alt="image-20230919113327294" style="zoom:67%;" />



## 4、堆叠卷积层

> - 对应下图，特征的低级到抽象概念
>
>   <img src="深度学习.assets/image-20230920090824394.png" alt="image-20230920090824394" style="zoom:80%;" />

<img src="深度学习.assets/image-20230920090746945.png" alt="image-20230920090746945" style="zoom:67%;" />



## 5、卷积层参数

![image-20230920091234396](深度学习.assets/image-20230920091234396.png)



### 5.1、步长

> - 步长不同会有什么影响？
>   - 会导致特征图大小不一，如下
> - 怎么选取合适步长？
>   - 如果在意图像的细节(需要高分辨率特征图)，就设置步长为1，而如果是为了加快训练更快地从图像中得出一些结论，就大步长

<img src="深度学习.assets/image-20230920091010985.png" alt="image-20230920091010985" style="zoom:67%;" />



### 5.2、卷积核尺寸

> - 小卷积核：适用于捕获局部特征和细节，有助于学习更多的局部模式，如边缘、纹理等。并且可以堆叠增加感受野
>
> - 大卷积核：用于捕获更大的区域信息，有助于学到更高级别的语义特征，如物体的整体形状。
> - 一般怎么用呢？
>   - 选择`小卷积核多层堆叠`，获得局部和全局信息的结合





### 5.3、边缘填充

> - 什么是边缘填充？
>
>   - 如下图，真正的图片只有紫色的部分，我给他外圈填充一圈0，相当于扩展吧
>
> - 为什么要边缘填充？
>
>   - 因为回顾卷积的过程，实际上越靠近中间的小块重复计算次数越高相当于自带权重就高一点，
>
>     为什么解决这种情况，只有`让边缘的点也多卷积几次，最直观的做法就是把它放中间去`，在边缘填充个0，反正0也不会影响最后的计算结果

<img src="深度学习.assets/image-20230920185624361.png" alt="image-20230920185624361" style="zoom:67%;" />





## 6、Calculate Process

<img src="深度学习.assets/image-20230921093627922.png" alt="image-20230921093627922" style="zoom:67%;" />

> - 涉及一个卷积参数的共享
>   - 相当于一组卷积核直接干满全部图片，emmm3个R/G/B每次滑动窗口移动的计算的卷积核都是相同的3个卷积核

<img src="深度学习.assets/image-20230921094115701.png" alt="image-20230921094115701" style="zoom:67%;" />



> - 粗讲
>
>   - 先卷积提取特征；然后RELU进行激活；再卷积在RELU；之后进行池化函数进行降维？
>
>   - 最后得到了一组高信息密度的特征，再进行全连接函数的操作，就前向-反向传播的操作了
>
>     相当于卷积就是在condense特征

<img src="深度学习.assets/image-20230921095828739.png" alt="image-20230921095828739" style="zoom:67%;" />

> - 可以看到越来越细分，也是越来越condense feature了吧

<img src="深度学习.assets/image-20230921095610397.png" alt="image-20230921095610397" style="zoom:67%;" />





## 7、池化层

> - 有些特征屁用没有，给它压缩一波
>
>   emmmm我现在反而是觉得，convolution status的时候，选出重要的characteristics

<img src="深度学习.assets/image-20230921094821258.png" alt="image-20230921094821258" style="zoom:67%;" />

> - Professional 说的选最大的最好

<img src="深度学习.assets/image-20230921094807211.png" alt="image-20230921094807211" style="zoom:67%;" /> 



## 8、残差网络Resnet

> - 啥啥啥啊这是？
>   - 见下面的解决方案：其实理解上不难，就是你现在有20层的卷积，你下一层的输入等于20层的卷积结果加上一些操作，而这些操作效果不好我就设置权重为0当没有，相当于保个底嘛

<img src="深度学习.assets/image-20230921231921950.png" alt="image-20230921231921950" style="zoom:67%;" />







## 9、感受野

> - 就是Second Conv的小窗口等于Fist Conv的整体综合，First Conv的每个小格子又等于它对应的那个滑动窗口的整体综合，相当于一层一层的在concentrate，最后一个小格子include all informations

<img src="深度学习.assets/image-20230922001322183.png" alt="image-20230922001322183" style="zoom:67%;" />

> - 为什么不选择一个大的卷积核呢？明明大的卷积核一次concentrate完了，跟小的卷积核condense一次又condense有啥区别？
>   - 堆叠小的卷积核所需的参数越少，故越好
>   - 不知道为啥，但是下面计算结果就是这样，哎但是这只针对很大的卷积核吧

<img src="深度学习.assets/image-20230922002337135.png" alt="image-20230922002337135" style="zoom:67%;" />







# 四、递归神经网络

## 1、OUTLINE

**Recursion Neural Network**

> - 就是：(t时刻input layer输入hidden layer产生的值) + (t+1时刻输入的input layer) = t+1时刻的总输入

<img src="深度学习.assets/image-20230928002916905.png" alt="image-20230928002916905" style="zoom:67%;" />

> - 在NLP中的应用，其实挺comprehensive的
>   - Exemplify：I am Chinsese，每个词都化为同一维度的向量(类似于jieba库的分词)，然后先输入I，再输入am，此时am和经过隐含层的I一起作为下一次输入，Chinese也是如此和前面的两个经过隐含层的单词一起输入，这样就能实现一起输入一整句话`(考虑全部输入)`，更加便于分析~

<img src="深度学习.assets/image-20230928003505267.png" alt="image-20230928003505267" style="zoom:67%;" />



## 2、LSTM网络

> - 为什么要用？
>   - 因为RNN的输出其实是基于前面全部的输入，这既繁重而且任意出问题
>   - 而LSTM可以实现控制模型的复杂度，也就是会discard一些信息
> - 怎么实现的？
>   - 有一个C参数控制单元，进行信息的filter

<img src="深度学习.assets/image-20230928004107418.png" alt="image-20230928004107418" style="zoom:67%;" />





## 3、词向量模型Word2Vec

> - 为什么要有？
>   - 很简单，将phrase进行量化才更好处理
>   - conventional处理文本方式只是关注frequency，但不关注语境。因此局限性太大，既然NLP咱们就等来点狠的
> - 是什么词向量？
>   - 很easy，首先我想先讲一下为什么要用向量，举个简单example：假设目前有两个词A和B，我可以用某种方法分别将他们投射到二维plane的某点去，而如果他们离得越近其实说明他们的关系越密切。
>     - 为什么离得越近越密切呢？这就看你你设置的两个axis了，每个axis代表他们的一个characteristic，既然feature都一样那么肯定越相似啦
> - 一般选用50-300维度(相当于这么多characteristics更加细致地筛选不同的phrase)





> - 实现`输入一个词，预测下一个词`的具体实现过程
>   - 就是先初始化一张大表，这张表的每一行元素=词的量化
>   - 我们每次输入都会对表的值进行更新，这样在n多次预测后，表也变得牛逼咯

<img src="深度学习.assets/image-20230928095003102.png" alt="image-20230928095003102" style="zoom:67%;" />



> - 构建训练数据
>
>   - 怎么构建？
>
>     - 就是搞一个窗口，令前两个word为输入，后一个word为输出
>
>     - 然后step为1继续滑动
>
>     - 重点
>
>       (1)`一句话也可以当做很多的数据训练很多次`
>
>       (2)体现了unsupervised，毕竟自己滑动自己训练
>
>   - 为什么output是第三个？
>
>     - 填词啊小老弟，根据context输出划线词汇

<img src="深度学习.assets/image-20230928100316862.png" alt="image-20230928100316862" style="zoom:67%;" /> 

> - 也可以通过中心词推测上下文

<img src="深度学习.assets/image-20231007093939526.png" alt="image-20231007093939526" style="zoom:67%;" /> 

> - 整体架构如下
>   - **CBOW**和**Skip-gram**模型

<img src="深度学习.assets/image-20231007094033998.png" alt="image-20231007094033998" style="zoom:67%;" />



> - 最后一层的softmax压力太大了，毕竟分类也不会说分成十万类嘛，如何解决呢？
>
>   那就得使用分级softmax分类器
>   
>   转换思路，本来是预测"not"后文单词可能是什么，现在变为"not"相邻的单词是否在前5000，如果是，则是否前2500......这样一个二叉树最后得到的结果就是相邻的单词所在的下标
>   
>   <img src="深度学习.assets/image-20231007162615617.png" alt="image-20231007162615617" style="zoom:67%;" />

<img src="深度学习.assets/image-20231007094836316.png" alt="image-20231007094836316" style="zoom:67%;" />

 



> - **负采样**(Negative Sampling)
>
>   - 什么是负采样
>
>     - 我们现在的逻辑就是，测试数据集的时候要根据已给参考样本来判断，举个例子，现在你知道了A后面是B，A后面不是C，那么我们就可以测试模型A-B、A-C的判断结果，参考已知样本，得出哪个判断结果错误，然后再去修正。
>
>       而A-C的target为0这种情况就是负样本，如果不给负样本，模型训练效果就不理想
>   
> - 为什么要有？
>
>   - formula推导的，不细究噜
>
>   - 数量多少合适？
>
>     - 5个合适，Gensim推荐的
>

<img src="深度学习.assets/image-20231007095340078.png" alt="image-20231007095340078" style="zoom:67%;" />



**训练完整流程**

![image-20231007100349493](深度学习.assets/image-20231007100349493.png)

<img src="深度学习.assets/image-20231007100415026.png" alt="image-20231007100415026" style="zoom:67%;" />

> - 

<img src="深度学习.assets/image-20231007100432190.png" alt="image-20231007100432190" style="zoom:67%;" />







# 五、GAN对抗神经网络

## 1、NOTION

> - 生成器生成certain stuff，判别器judge real/ture
>   - 两者相互对抗，相互优化

<img src="深度学习.assets/image-20231016183207283.png" alt="image-20231016183207283" style="zoom:67%;" />























`

`

`

`

`

